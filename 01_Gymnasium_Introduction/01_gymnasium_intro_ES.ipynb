{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La Verdad sobre Convertirse en un Ingeniero de Aprendizaje Automático (ML)\n",
    "\n",
    "¡El aprendizaje automático es increíblemente fascinante y divertido! Sin embargo, un juego que compras en la tienda no estará diseñado para ser jugado por una computadora (algoritmo de ML), lo que nos lleva a un problema y probablemente a una de las partes más intensivas en tiempo de ser un ingeniero de ML: alimentar las entradas de un entorno (virtual o real) en nuestro algoritmo.\n",
    "\n",
    "Afortunadamente, existe una biblioteca de Python (un paquete de código) que ya existe y eliminará este problema por nosotros!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gymnasium-text.png\" alt=\"gymnasium\" width=\"400\"/>\n",
    "\n",
    "# ________________________________________________________\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"adventure.gif\" width=\"98\" />\n",
    "  <img src=\"car_racing.gif\" width=\"225\" /> \n",
    "  <img src=\"frozen_lake.gif\" width=\"150\" />\n",
    "  <img src=\"taxi.gif\" width=\"235\" />\n",
    "</p>\n",
    "\n",
    "Gymnasium, de OpenAI, los creadores de ChatGPT, nos dará acceso a un número infinito de entornos, permitiéndonos probar y entrenar nuestros futuros algoritmos de ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"left\">\n",
    "    <img src=\"AE_loop (1).png\" alt=\"drawing\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gymnasium implementa el bucle clásico de agente y entorno (como se ve arriba) utilizado para el aprendizaje por refuerzo. El Agente (o Algoritmo de Aprendizaje por Refuerzo RL) obtiene una observación inicial del entorno (su \"Estado\") luego el agente realiza una acción (\"Acción\") y obtiene de vuelta una observación (\"Estado\") y una recompensa (\"Recompensa\"). La recompensa se otorga cuando el Agente alcanza un objetivo o subobjetivo y esto fomenta o refuerza el aprendizaje (de allí el nombre de aprendizaje por refuerzo). El Agente también puede obtener información adicional, como si el juego terminó o se detuvo prematuramente (\"Terminado\" o \"Truncado\").\n",
    "\n",
    "Términos:\n",
    "- **Agente**: Nuestro algoritmo de aprendizaje por refuerzo (o nosotros si estamos jugando el juego)\n",
    "- **Entorno**: El mundo virtual con el que interactúa el Agente\n",
    "- **Acción**: La acción realizada sobre el entorno\n",
    "- **Estado**: La posición actual del Agente o lo que puede observar actualmente.\n",
    "- **Recompensa**: La recompensa dada por ganar el juego o realizar una acción deseada.\n",
    "- **Terminado**: Juego terminado por éxito o fracaso.\n",
    "- **Truncado**: Juego terminado inesperadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Inicializar Nuestro Entorno\n",
    "\n",
    "Primero necesitamos importar el paquete gymnasium\n",
    "\n",
    "``` python\n",
    "import gymnasium as gym\n",
    "```\n",
    "\n",
    "Luego necesitamos inicializar nuestro objeto de entorno\n",
    "\n",
    "``` python\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "```\n",
    "\n",
    "Este objeto \"env\" es con lo que interactuaremos para recopilar información sobre nuestro entorno de juego. gym.make() toma un argumento que le dice a gymnasium qué juego quieres cargar. En este caso es Cart Pole (un juego de equilibrio).\n",
    "\n",
    "### Ejercicio 1: Inicializar el entorno Cliff Walking\n",
    "\n",
    "Inicializa el entorno como se muestra con el juego Cliff Walking. Mira un gif del entorno a continuación.\n",
    "\n",
    "<img src=\"cliff_walking.gif\" width=\"400\" />\n",
    "\n",
    "``` python\n",
    "\"CliffWalking-v0\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Obtener observaciones iniciales\n",
    "\n",
    "Luego, usa el método reset() para obtener observaciones iniciales (o \"Estado\") del objeto de entorno (\"env\")\n",
    "\n",
    "También devolverá información que no utilizaremos.\n",
    "\n",
    "``` python\n",
    "state, info = env.reset()\n",
    "```\n",
    "\n",
    "### Ejercicio 2: Obtener e imprimir la observación inicial\n",
    "\n",
    "En el siguiente bloque de código, obtén el estado e imprímelo con el método print() que usamos en nuestro tutorial de python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota que se devolvió un número (o entero int). Este int corresponde a la posición del personaje en este entorno, que resulta ser un mundo de cuadrícula con cada espacio teniendo un número como se muestra a continuación. ¿Puedes ver dónde está el personaje?\n",
    "\n",
    "|  0  |  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |  9  | 10 | 11 |\n",
    "|---|-----|-----|-----|-----|-----|-----|-----|-----|-----|----|----|\n",
    "| 12  | 13  | 14  | 15  | 16  | 17  | 18  | 19  | 20  | 21  | 22 | 23 |\n",
    "| 24  | 25  | 26  | 27  | 28  | 29  | 30  | 31  | 32  | 33  | 34 | 35 |\n",
    "| 36  | 37  | 38  | 39  | 40  | 41  | 42  | 43  | 44  | 45  | 46 | 47 |\n",
    "\n",
    "Podemos obtener una imagen del estado actual del juego obteniendo primero una matriz de colores rgb y luego mostrándola con una biblioteca simple llamada matplotlib.\n",
    "\n",
    "``` python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rgb_array = env.render(mode='rgb_array')\n",
    "\n",
    "plt.imshow(rgb_array)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Dar un Paso en una Dirección Específica\n",
    "\n",
    "Ahora que hemos inicializado nuestro entorno, es hora de comenzar a interactuar con él. En lugar de dejar que la computadora elija acciones al azar, tú serás quien decida en qué dirección moverse. Piensa en ti mismo como el agente en el juego, tratando de navegar a través del entorno.\n",
    "\n",
    "### Acciones que Puedes Realizar:\n",
    "\n",
    "- `0`: Mover hacia arriba\n",
    "- `1`: Mover a la derecha\n",
    "- `2`: Mover hacia abajo\n",
    "- `3`: Mover a la izquierda\n",
    "\n",
    "Cada acción resultará en una nueva observación del entorno, una recompensa por la acción, y una indicación de si el episodio (juego) ha terminado.\n",
    "\n",
    "### Ejemplo 3: Dar un Paso a la Derecha\n",
    "\n",
    "Vamos a empezar dando un paso hacia la derecha. Aquí te explico cómo hacerlo:\n",
    "\n",
    "```python\n",
    "# Define la acción como moverse a la derecha\n",
    "action = 1  # 1 significa moverse a la derecha\n",
    "\n",
    "# Da un paso en la dirección elegida\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# Muestra el estado, la recompensa y si el juego ha terminado\n",
    "print(state, reward, terminated)\n",
    "\n",
    "# Muestra la imagen si lo deseas\n",
    "rgb_array = env.render()\n",
    "plt.imshow(rgb_array)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "En este ejemplo, estás moviendo al agente a la derecha. El entorno te dirá cómo se ve después del movimiento (`observación`), qué tan bueno o malo fue el movimiento (`recompensa`), si el juego ha terminado (`terminado`), y cualquier información adicional (`info`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lograste mover al personaje a la derecha, probablemente estés muerto, lo que significa que deberías haber recibido una recompensa de -1. Además, después de morir, tu personaje es movido nuevamente a la posición inicial 36.\n",
    "\n",
    "## Paso 4: ¡Obtén la recompensa!\n",
    "\n",
    "Ahora tienes suficiente conocimiento para guiar al jugador hasta la meta y obtener la recompensa. ¡Inténtalo!\n",
    "\n",
    "Recuerda:\n",
    "\n",
    "- `0`: Mover hacia arriba\n",
    "- `1`: Mover a la derecha\n",
    "- `2`: Mover hacia abajo\n",
    "- `3`: Mover a la izquierda\n",
    "\n",
    "Pista: puedes copiar y pegar tu código anterior para hacer pasos repetidos, además de que puedes usar un bucle para ahorrar tiempo escribiendo código.\n",
    "\n",
    "Ejemplo de bucle:\n",
    "``` python\n",
    "for i in range(5):\n",
    "    # Haz algo 5 veces\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
