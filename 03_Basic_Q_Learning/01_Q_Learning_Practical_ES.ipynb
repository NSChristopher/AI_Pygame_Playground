{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Explicación de Q-Learning\n",
    "\n",
    "### Introducción: De la Búsqueda Aleatoria con Memoria a Q-Learning\n",
    "\n",
    "Comenzaste con una búsqueda aleatoria con memoria, donde el agente recordaba caminos que funcionaban. Esto funcionaba bien en situaciones simples, pero tenía dificultades cuando el juego se volvía complicado, como cuando el Lago Congelado se volvía resbaladizo. Si el agente resbalaba, el camino recordado se volvía inútil y el agente podría confundirse o fallar.\n",
    "\n",
    "### ¿Qué Hace Diferente a Q-Learning?\n",
    "\n",
    "Q-Learning es más inteligente porque no solo memoriza caminos; aprende a tomar buenas decisiones en cualquier situación:\n",
    "\n",
    "- **Generalización**: Q-Learning ayuda al agente a descubrir los mejores movimientos, incluso si las cosas no salen como se planeó, como resbalarse en el hielo.\n",
    "  \n",
    "- **Adaptabilidad**: El agente puede ajustarse a los cambios y seguir tomando decisiones inteligentes, incluso si termina en un lugar diferente al esperado.\n",
    "\n",
    "- **Aprendizaje a Partir de la Experiencia**: El agente aprende de cada movimiento, gane o pierda, mejorando cada vez que juega.\n",
    "\n",
    "### Fundamentos de Q-Learning\n",
    "\n",
    "Q-Learning ayuda al agente a mejorar en un juego utilizando una tabla Q, que es como una hoja de trucos. La tabla comienza vacía, pero a medida que el agente juega, se llena con puntuaciones que muestran qué movimientos son buenos y cuáles no lo son.\n",
    "\n",
    "#### La Ecuación de Aprendizaje\n",
    "\n",
    "El agente actualiza la tabla Q usando esta ecuación:\n",
    "\n",
    "```python\n",
    "q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "    reward + discount_rate * np.max(q_table[new_state, :]) - q_table[state, action]\n",
    ")\n",
    "```\n",
    "\n",
    "Esta ecuación ayuda al agente a decidir cómo mejorar su hoja de trucos en función de lo que sucedió después de realizar un movimiento.\n",
    "\n",
    "#### Exploración vs. Explotación\n",
    "\n",
    "Al principio, el agente intenta movimientos aleatorios para aprender qué funciona (exploración). A medida que se vuelve más inteligente, usa lo que sabe para tomar mejores decisiones (explotación). Con el tiempo, explora menos y se basa más en lo que ha aprendido. Así es como se ve esto en el código:\n",
    "\n",
    "```python\n",
    "epsilon = 1  # Comenzar con exploración completa\n",
    "epsilon_decay_rate = 0.0001  # Disminuir lentamente la exploración\n",
    "rand_num_gen = np.random.default_rng()\n",
    "\n",
    "# Durante el juego\n",
    "if rand_num_gen.random() < epsilon:\n",
    "    action = env.action_space.sample()  # Explorar: elegir una acción aleatoria\n",
    "else:\n",
    "    action = np.argmax(q_table[state, :])  # Explotar: elegir la mejor acción conocida\n",
    "```\n",
    "\n",
    "A medida que el agente aprende, el valor de `epsilon` disminuye:\n",
    "\n",
    "```python\n",
    "epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "```\n",
    "\n",
    "Esto significa que el agente explora menos y comienza a tomar decisiones basadas en lo que ha aprendido.\n",
    "\n",
    "### Funciones de Numpy en Q-Learning\n",
    "\n",
    "- **np.zeros()**: Crea la tabla Q, comenzando con ceros.\n",
    "\n",
    "```python\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])  # Inicializar tabla Q\n",
    "```\n",
    "\n",
    "- **rand_num_gen.random()**: Ayuda al agente a decidir si explora o explota.\n",
    "\n",
    "```python\n",
    "if rand_num_gen.random() < epsilon:\n",
    "    action = env.action_space.sample()  # Acción aleatoria para exploración\n",
    "```\n",
    "\n",
    "- **np.argmax()**: Encuentra el mejor movimiento basado en lo que ha aprendido el agente.\n",
    "\n",
    "```python\n",
    "action = np.argmax(q_table[state, :])  # Mejor acción conocida para explotación\n",
    "```\n",
    "\n",
    "- **np.max()**: Mira hacia adelante para encontrar la mejor recompensa posible desde la nueva posición del agente.\n",
    "\n",
    "```python\n",
    "reward + discount_rate * np.max(q_table[new_state, :])\n",
    "```\n",
    "\n",
    "# Asignación: implementar la ecuación de actualización de la tabla Q y ajustar los hiperparámetros\n",
    "\n",
    "A continuación se muestra el código incompleto para un algoritmo de Q-learning. Para esta asignación, primero debes implementar la ecuación de Q-learning.\n",
    "\n",
    "En segundo lugar, ajusta los hiperparámetros y observa los resultados ejecutando las visualizaciones en las últimas celdas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Setup Frozen Lake environment\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
    "\n",
    "# Initialize variables for visualization\n",
    "images = []\n",
    "q_table_states = []\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters with reasonable tuning ranges\n",
    "learning_rate = 0.1  # Learning rate (0.1 to 0.9)\n",
    "discount_rate = 0.8  # Discount rate (0.8 to 0.99)\n",
    "epsilon = 1  # Exploration rate (0.1 to 1)\n",
    "epsilon_decay_rate = 0.009  # Exploration decay rate (0.0001 to 0.01)\n",
    "rand_num_gen = np.random.default_rng()\n",
    "episodes = 1000  # Number of episodes (500 to 10,000)\n",
    "\n",
    "# Track rewards per episode\n",
    "rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    terminated, truncated = False, False\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        # Exploration vs. Exploitation\n",
    "        if rand_num_gen.random() < epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])  # Exploit\n",
    "\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # Step 1: Implement Q-learning equation\n",
    "\n",
    "        # Visualization save only the first and last 100 episodes\n",
    "        if episode <= 100 or episode > episodes - 100:\n",
    "            images.append(env.render())\n",
    "            q_table_states.append(q_table.copy())\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    # Decay exploration rate\n",
    "    epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "\n",
    "    # Slow down learning when exploration ends\n",
    "    if epsilon == 0:\n",
    "        learning_rate = 0.0001\n",
    "\n",
    "    # Track rewards for successful episodes\n",
    "    if reward == 1:\n",
    "        rewards_per_episode[episode] = 1\n",
    "\n",
    "# Calculate the sum of rewards over the last 100 episodes\n",
    "sum_rewards = np.zeros(episodes)\n",
    "for t in range(episodes):\n",
    "    sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-100):(t+1)])\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(sum_rewards)\n",
    "plt.savefig('frozen_lake8x8.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.plotting import plot_q_values_grid\n",
    "\n",
    "plot_q_values_grid(q_table, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Agrega el directorio principal a sys.path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.plotting import display_video\n",
    "\n",
    "video = display_video(images[-20:], interval=50)\n",
    "HTML(video.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
