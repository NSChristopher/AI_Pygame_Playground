{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Q-Learning Explanation\n",
    "\n",
    "### Introduction: From Random Search with Memory to Q-Learning\n",
    "\n",
    "You started with random search with memory, where the agent remembered paths that worked. This was okay for simple situations, but it struggled when the game got tricky—like when the Frozen Lake became slippery. If the agent slipped, the remembered path became useless, and the agent might get confused or crash.\n",
    "\n",
    "### What Makes Q-Learning Different?\n",
    "\n",
    "Q-Learning is smarter because it doesn't just memorize paths; it learns to make good decisions in any situation:\n",
    "\n",
    "- **Generalization**: Q-Learning helps the agent figure out the best moves, even if things don't go as planned, like slipping on ice.\n",
    "  \n",
    "- **Adaptability**: The agent can adjust to changes and still make smart choices, even if it ends up in a different spot than expected.\n",
    "\n",
    "- **Learning from Experience**: The agent learns from every move, win or lose, getting better each time it plays.\n",
    "\n",
    "### Q-Learning Basics\n",
    "\n",
    "Q-Learning helps the agent get better at a game by using a Q-table, which is like a cheat sheet. The table starts empty, but as the agent plays, it fills in with scores that show which moves are good and which aren't.\n",
    "\n",
    "#### The Learning Equation\n",
    "\n",
    "The agent updates the Q-table using this equation:\n",
    "\n",
    "```python\n",
    "q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "    reward + discount_rate * np.max(q_table[new_state, :]) - q_table[state, action]\n",
    ")\n",
    "```\n",
    "\n",
    "This equation helps the agent decide how to improve its cheat sheet based on what happened after it made a move.\n",
    "\n",
    "#### Exploration vs. Exploitation\n",
    "\n",
    "At first, the agent tries random moves to learn what works (exploration). As it gets smarter, it uses what it knows to make better choices (exploitation). Over time, it explores less and relies more on its learning. Here's how this looks in the code:\n",
    "\n",
    "```python\n",
    "epsilon = 1  # Start with full exploration\n",
    "epsilon_decay_rate = 0.0001  # Slowly decrease exploration\n",
    "rand_num_gen = np.random.default_rng()\n",
    "\n",
    "# During the game\n",
    "if rand_num_gen.random() < epsilon:\n",
    "    action = env.action_space.sample()  # Explore: choose a random action\n",
    "else:\n",
    "    action = np.argmax(q_table[state, :])  # Exploit: choose the best-known action\n",
    "```\n",
    "\n",
    "As the agent learns, the `epsilon` value decreases:\n",
    "\n",
    "```python\n",
    "epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "```\n",
    "\n",
    "This means the agent explores less and starts making decisions based on what it has learned.\n",
    "\n",
    "### Numpy Functions in Q-Learning\n",
    "\n",
    "- **np.zeros()**: Creates the Q-table, starting with zeros.\n",
    "\n",
    "```python\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])  # Initialize Q-table\n",
    "```\n",
    "\n",
    "- **rand_num_gen.random()**: Helps the agent decide whether to explore or exploit.\n",
    "\n",
    "```python\n",
    "if rand_num_gen.random() < epsilon:\n",
    "    action = env.action_space.sample()  # Random action for exploration\n",
    "```\n",
    "\n",
    "- **np.argmax()**: Finds the best move based on what the agent has learned.\n",
    "\n",
    "```python\n",
    "action = np.argmax(q_table[state, :])  # Best known action for exploitation\n",
    "```\n",
    "\n",
    "- **np.max()**: Looks ahead to find the best possible reward from the agent's new position.\n",
    "\n",
    "```python\n",
    "reward + discount_rate * np.max(q_table[new_state, :])\n",
    "```\n",
    "\n",
    "# Assignment: implement the q table update equation and adjust hyper perameters\n",
    "\n",
    "Bellow is the incomplete code for a q learning algorythm. for this assignment you need to first implement the q learning equation.\n",
    "\n",
    "Secondly adjust the hyperperameters and notice the results by running the visuallizations in the last few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Configurar el entorno Frozen Lake\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False, render_mode='rgb_array')\n",
    "\n",
    "# Inicializar variables para visualización\n",
    "images = []\n",
    "q_table_states = []\n",
    "\n",
    "# Inicializar la tabla Q con ceros\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hiperparámetros con rangos de ajuste razonables\n",
    "learning_rate = 0.1  # Tasa de aprendizaje (0.1 a 0.9)\n",
    "discount_rate = 0.8  # Tasa de descuento (0.8 a 0.99)\n",
    "epsilon = 1  # Tasa de exploración (0.1 a 1)\n",
    "epsilon_decay_rate = 0.009  # Tasa de disminución de la exploración (0.0001 a 0.01)\n",
    "rand_num_gen = np.random.default_rng()\n",
    "episodes = 1000  # Número de episodios (500 a 10,000)\n",
    "\n",
    "# Seguimiento de recompensas por episodio\n",
    "rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    terminated, truncated = False, False\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        # Exploración vs. Explotación\n",
    "        if rand_num_gen.random() < epsilon:\n",
    "            action = env.action_space.sample()  # Explorar\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])  # Explotar\n",
    "\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # Paso 1: Implementar la ecuación de Q-learning\n",
    "\n",
    "        # Guardar visualización solo en los primeros y últimos 100 episodios\n",
    "        if episode <= 100 or episode > episodes - 100:\n",
    "            images.append(env.render())\n",
    "            q_table_states.append(q_table.copy())\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    # Reducir la tasa de exploración\n",
    "    epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "\n",
    "    # Reducir la tasa de aprendizaje cuando la exploración termina\n",
    "    if epsilon == 0:\n",
    "        learning_rate = 0.0001\n",
    "\n",
    "    # Seguimiento de recompensas para episodios exitosos\n",
    "    if reward == 1:\n",
    "        rewards_per_episode[episode] = 1\n",
    "\n",
    "# Calcular la suma de recompensas en los últimos 100 episodios\n",
    "sum_rewards = np.zeros(episodes)\n",
    "for t in range(episodes):\n",
    "    sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-100):(t+1)])\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(sum_rewards)\n",
    "plt.savefig('frozen_lake8x8.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.plotting import plot_q_values_grid\n",
    "\n",
    "plot_q_values_grid(q_table, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Agrega el directorio principal a sys.path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.plotting import display_video\n",
    "\n",
    "video = display_video(images[-20:], interval=50)\n",
    "HTML(video.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
