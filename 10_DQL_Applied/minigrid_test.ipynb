{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Custom Minigrid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#10_DQL_Applied/Minigrid_with_Monsters\n",
    "\n",
    "# Agrega el directorio principal a sys.path\n",
    "module_path = os.path.abspath(os.path.join('./Minigrid_with_Monsters'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "from Minigrid_with_Monsters.minigrid.core.constants import COLOR_NAMES\n",
    "from Minigrid_with_Monsters.minigrid.core.grid import Grid\n",
    "from Minigrid_with_Monsters.minigrid.core.mission import MissionSpace\n",
    "from Minigrid_with_Monsters.minigrid.core.world_object import Goal\n",
    "from Minigrid_with_Monsters.minigrid.minigrid_env import MiniGridEnv\n",
    "from Minigrid_with_Monsters.minigrid.core.world_object import Monster\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class LockedRoomEnv(MiniGridEnv):\n",
    "    def __init__(self, size=19, max_steps: int | None = None, **kwargs):\n",
    "        self.size = size\n",
    "        self.monster = Monster()\n",
    "        self.goal_pos = None\n",
    "\n",
    "        if max_steps is None:\n",
    "            max_steps = 10 * size\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"Go to the green goal square avoid the red monster.\")\n",
    "        \n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            width=size,\n",
    "            height=size,\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        # Create the grid\n",
    "        self.grid = Grid(width, height)\n",
    "\n",
    "        # Generate the surrounding walls\n",
    "        self.grid.horz_wall(0, 0)\n",
    "        self.grid.horz_wall(0, height - 1)\n",
    "        self.grid.vert_wall(0, 0)\n",
    "        self.grid.vert_wall(width - 1, 0)\n",
    "\n",
    "        room_w = width // 2\n",
    "        room_h = height // 2\n",
    "\n",
    "        # For each row of rooms\n",
    "        for j in range(0, 2):\n",
    "            # For each column\n",
    "            for i in range(0, 2):\n",
    "                xL = i * room_w\n",
    "                yT = j * room_h\n",
    "                xR = xL + room_w\n",
    "                yB = yT + room_h\n",
    "\n",
    "                # Bottom wall and door\n",
    "                if i + 1 < 2:\n",
    "                    self.grid.vert_wall(xR, yT, room_h)\n",
    "                    pos = (xR, self._rand_int(yT + 1, yB))\n",
    "                    self.grid.set(*pos, None)\n",
    "\n",
    "                # Bottom wall and door\n",
    "                if j + 1 < 2:\n",
    "                    self.grid.horz_wall(xL, yB, room_w)\n",
    "                    pos = (self._rand_int(xL + 1, xR), yB)\n",
    "                    self.grid.set(*pos, None)\n",
    "\n",
    "        # Place the agent goal and monster\n",
    "        self.place_agent()\n",
    "        self.goal_pos = self.place_obj(Goal())\n",
    "        self.monster.position = self.place_obj(self.monster)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        # Check if monster can see the agent. If so, move towards it.\n",
    "        # If the monster is in the same cell as the agent, the agent is caught.\n",
    "        if self.monster.can_see(self.grid, self.agent_pos):\n",
    "            # if the monster can see the agent, move towards it\n",
    "            self.monster.move_towards(self.grid, self.agent_pos)\n",
    "\n",
    "            if self.monster.position == self.agent_pos:\n",
    "                reward = -.5\n",
    "                terminated = True\n",
    "        else:\n",
    "            # if the monster can't see the agent, move randomly\n",
    "            self.monster.patrol_forward(self.grid)\n",
    "\n",
    "        self.grid.set(*self.monster.position, self.monster)\n",
    "\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = 1\n",
    "            terminated = True\n",
    "\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Play**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.play import play\n",
    "import numpy as np\n",
    "\n",
    "env = LockedRoomEnv(render_mode=\"rgb_array\", max_steps=1000)\n",
    "\n",
    "play(env, keys_to_action={\n",
    "    (ord(\"a\"),): 0,\n",
    "    (ord(\"d\"),): 1,\n",
    "    (ord(\"w\"),): 2,\n",
    "    (ord(\"j\"),): 3,\n",
    "    (ord(\"k\"),): 5,\n",
    "}, noop=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **El Modelo PPO: Cómo Ayuda al Agente a Jugar el Juego MiniGrid**\n",
    "\n",
    "El modelo que usaremos para jugar este juego de MiniGrid se llama **Optimización de Política Proximal (PPO)**. Es un algoritmo popular de aprendizaje automático que enseña a un agente cómo navegar por una cuadrícula, alcanzar un objetivo y evitar obstáculos aprendiendo de sus experiencias.\n",
    "\n",
    "## **1. El Extractor de Características**\n",
    "\n",
    "### **¿Qué es un Extractor de Características?**\n",
    "Antes de que el agente pueda tomar decisiones, necesita entender lo que está viendo en la cuadrícula. Un **extractor de características** ayuda con esto al enfocarse en los detalles importantes del entorno.\n",
    "\n",
    "### **¿Cómo Funciona?**\n",
    "- El extractor de características utiliza **Redes Neuronales Convolucionales (CNNs)**, que están especializadas en analizar datos visuales.\n",
    "- **Las CNNs** funcionan escaneando una imagen para detectar patrones, como bordes o formas, que son cruciales para entender la disposición de la cuadrícula.\n",
    "\n",
    "### **Por Qué Es Importante:**\n",
    "Este paso simplifica los datos visuales, facilitando que el agente aprenda y tome decisiones inteligentes en el juego.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MinigridFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super(MinigridFeaturesExtractor, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        return self.linear(self.cnn(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=MinigridFeaturesExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Inicialización de PPO**\n",
    "\n",
    "### **¿Qué es la Inicialización de PPO?**\n",
    "Ahora que el agente puede entender su entorno, configuramos el modelo **Optimización de Política Proximal (PPO)**, que guiará las acciones del agente.\n",
    "\n",
    "### **¿Qué Sucede Aquí?**\n",
    "- **Política:** Elegimos una **CnnPolicy**, lo que significa que las decisiones del agente se basan en los datos visuales procesados por la CNN.\n",
    "- **Extractor de Características:** Las características procesadas por la CNN se introducen en el modelo PPO para ayudar al agente a decidir su próximo movimiento.\n",
    "- **Hiperparámetros:** Se ajustan configuraciones importantes como la tasa de aprendizaje para controlar qué tan rápido aprende el agente.\n",
    "\n",
    "### **Por Qué Es Importante:**\n",
    "Esta configuración le da al agente la capacidad de aprender de sus experiencias y mejorar su desempeño en el juego con el tiempo.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Entrenamiento del Modelo PPO**\n",
    "\n",
    "### **¿Qué es el Entrenamiento?**\n",
    "Con el modelo configurado, es hora de entrenar al agente. **El entrenamiento** es donde el agente aprende interactuando con la cuadrícula, probando diferentes acciones y descubriendo qué funciona mejor.\n",
    "\n",
    "### **¿Cómo Funciona?**\n",
    "- El agente realiza movimientos y recibe recompensas o penalizaciones según sus acciones.\n",
    "- A lo largo de muchos intentos, el agente aprende las mejores estrategias para maximizar sus recompensas.\n",
    "- El entrenamiento continúa hasta que el agente se convierte en un experto en alcanzar el objetivo.\n",
    "\n",
    "### **Por Qué Es Importante:**\n",
    "El entrenamiento transforma al agente de un principiante a un profesional en jugar el juego MiniGrid, ayudándole a tomar mejores decisiones y alcanzar sus objetivos de manera constante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 315      |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 973      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 274         |\n",
      "|    ep_rew_mean          | -0.214      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 707         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012304688 |\n",
      "|    clip_fraction        | 0.0346      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.013034642 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0209     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00256    |\n",
      "|    value_loss           | 0.00485     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 245         |\n",
      "|    ep_rew_mean          | -0.24       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01118236  |\n",
      "|    clip_fraction        | 0.0781      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | -0.21764195 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0199     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    value_loss           | 0.00422     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 213          |\n",
      "|    ep_rew_mean          | -0.197       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 632          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.012096129  |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.91        |\n",
      "|    explained_variance   | -0.031090498 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00265     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00974     |\n",
      "|    value_loss           | 0.00977      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 215          |\n",
      "|    ep_rew_mean          | -0.245       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 618          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.010961829  |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.89        |\n",
      "|    explained_variance   | -0.019353509 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0045       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00835     |\n",
      "|    value_loss           | 0.0188       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 212         |\n",
      "|    ep_rew_mean          | -0.25       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008475232 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | -0.3471713  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0364     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00923    |\n",
      "|    value_loss           | 0.00516     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 215          |\n",
      "|    ep_rew_mean          | -0.246       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 604          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.010596022  |\n",
      "|    clip_fraction        | 0.116        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.86        |\n",
      "|    explained_variance   | 0.0077673197 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0232      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00848     |\n",
      "|    value_loss           | 0.00878      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 217          |\n",
      "|    ep_rew_mean          | -0.253       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 599          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125087155 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.84        |\n",
      "|    explained_variance   | -0.13557851  |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00449      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0146      |\n",
      "|    value_loss           | 0.00788      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 220         |\n",
      "|    ep_rew_mean          | -0.262      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 596         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010743049 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | -0.4045509  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0141     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00999    |\n",
      "|    value_loss           | 0.0038      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 221         |\n",
      "|    ep_rew_mean          | -0.261      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 593         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009936897 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | -0.14391422 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.015      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 0.00304     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 233          |\n",
      "|    ep_rew_mean          | -0.25        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 591          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0141295735 |\n",
      "|    clip_fraction        | 0.174        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.81        |\n",
      "|    explained_variance   | 0.1490531    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0132      |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0139      |\n",
      "|    value_loss           | 0.00804      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 217         |\n",
      "|    ep_rew_mean          | -0.275      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 589         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013367922 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | -6.209449   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0304     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 0.000431    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 216         |\n",
      "|    ep_rew_mean          | -0.255      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 588         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012644244 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0.020181179 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0206     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.0104      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 220         |\n",
      "|    ep_rew_mean          | -0.245      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 586         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012975103 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | 0.07407081  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0252     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 0.00864     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 231         |\n",
      "|    ep_rew_mean          | -0.245      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 585         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011555024 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | 0.26980978  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0395     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 0.0114      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 235         |\n",
      "|    ep_rew_mean          | -0.235      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 584         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015046634 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | -0.76117754 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 0.00179     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 241         |\n",
      "|    ep_rew_mean          | -0.22       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 583         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012104268 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | -0.14660788 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0303     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 0.00522     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 237         |\n",
      "|    ep_rew_mean          | -0.19       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 583         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01499776  |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.81       |\n",
      "|    explained_variance   | 0.029470086 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0396     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 0.00663     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 234         |\n",
      "|    ep_rew_mean          | -0.18       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 582         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017963197 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | 0.04522109  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0526     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 0.0127      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 237         |\n",
      "|    ep_rew_mean          | -0.18       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 581         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012840712 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.15859729  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0078      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 0.0077      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 236         |\n",
      "|    ep_rew_mean          | -0.15       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 581         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017667066 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.77       |\n",
      "|    explained_variance   | -0.3759477  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.042      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 0.0039      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 233         |\n",
      "|    ep_rew_mean          | -0.165      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 580         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017955225 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.044980466 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.043      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 0.00828     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 229         |\n",
      "|    ep_rew_mean          | -0.155      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 580         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012121299 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | -0.9865563  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0357     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 0.00299     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 225         |\n",
      "|    ep_rew_mean          | -0.19       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 579         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012556354 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | 0.001981318 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0167     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00998    |\n",
      "|    value_loss           | 0.00553     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 224         |\n",
      "|    ep_rew_mean          | -0.205      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 579         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014409286 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.69       |\n",
      "|    explained_variance   | 0.018343031 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0269     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 0.0181      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 188        |\n",
      "|    ep_rew_mean          | -0.22      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 578        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 91         |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01918779 |\n",
      "|    clip_fraction        | 0.209      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.7       |\n",
      "|    explained_variance   | -0.2700708 |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0332    |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    value_loss           | 0.00548    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 186         |\n",
      "|    ep_rew_mean          | -0.215      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 578         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018210245 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.68       |\n",
      "|    explained_variance   | 0.025014997 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0179     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.0181      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 190          |\n",
      "|    ep_rew_mean          | -0.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 578          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.016067546  |\n",
      "|    clip_fraction        | 0.191        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -0.104344964 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0283      |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.0139      |\n",
      "|    value_loss           | 0.0108       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 158         |\n",
      "|    ep_rew_mean          | -0.18       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 577         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 102         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016701765 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.15701765  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 0.0112      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 137         |\n",
      "|    ep_rew_mean          | -0.13       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 577         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01763539  |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.086274266 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0267     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 0.0222      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 155         |\n",
      "|    ep_rew_mean          | -0.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 577         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019963479 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.15593296  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0273     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 0.0169      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 161        |\n",
      "|    ep_rew_mean          | -0.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 577        |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 113        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01908544 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.45      |\n",
      "|    explained_variance   | 0.24276549 |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0273    |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    value_loss           | 0.0094     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 168         |\n",
      "|    ep_rew_mean          | -0.09       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 576         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 117         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013787493 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.0325948   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0202     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 0.00851     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 172         |\n",
      "|    ep_rew_mean          | -0.095      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 576         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014950651 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -0.40272486 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0125     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 162         |\n",
      "|    ep_rew_mean          | -0.06       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 576         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 124         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018105585 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.077011704 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00651     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00737    |\n",
      "|    value_loss           | 0.0124      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 163         |\n",
      "|    ep_rew_mean          | -0.045      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 576         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 127         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014882774 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.20772338  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0215     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 0.0184      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 174         |\n",
      "|    ep_rew_mean          | -0.045      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 575         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015885543 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.10336304  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00474     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 0.0193      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 191         |\n",
      "|    ep_rew_mean          | -0.065      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 575         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016841479 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | -0.76368463 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0391     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 0.00313     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 211         |\n",
      "|    ep_rew_mean          | -0.075      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 575         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017259944 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | -0.23284233 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0394     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 0.00378     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 213         |\n",
      "|    ep_rew_mean          | -0.125      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 575         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017493434 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | -0.15631318 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0309     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 0.00288     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x78cd9c46a900>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from Minigrid_with_Monsters.minigrid.wrappers import ImgObsWrapper\n",
    "\n",
    "# Initialize the environment\n",
    "env = LockedRoomEnv(max_steps=500)\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "# Initialize the model with custom feature extractor\n",
    "model = PPO(\n",
    "    \"CnnPolicy\", \n",
    "    env, \n",
    "    policy_kwargs=policy_kwargs, \n",
    "    learning_rate=2e-4,  # Lower the learning rate\n",
    "    gamma=0.99,          # Adjust the discount factor\n",
    "    clip_range=0.1,      # Lower the clip range\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.learn(8e4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Generar un Video y Mejorar el Modelo**\n",
    "\n",
    "En la siguiente sección, vamos a generar un video que muestra cómo el agente se desempeña en el entorno MiniGrid. \n",
    "\n",
    "Mira el video para observar cómo actúa el agente. Después, intenta entrenar el modelo nuevamente para ver si puedes mejorar su rendimiento. ¡Recuerda que el aprendizaje es un proceso continuo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Create the environment and wrap it again for rendering\n",
    "env = LockedRoomEnv(render_mode=\"rgb_array\", max_steps=500)\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "# Load the trained model\n",
    "#model = PPO.load(\"locked_room_model.zip\")\n",
    "\n",
    "# Reset the environment and get the initial observation\n",
    "obs, _ = env.reset()  # Extract only the observation\n",
    "\n",
    "done = False\n",
    "\n",
    "# Set up the video writer (using OpenCV)\n",
    "frame_width = env.render().shape[1]\n",
    "frame_height = env.render().shape[0]\n",
    "video = cv2.VideoWriter('ppo_agent_performance.avi', cv2.VideoWriter_fourcc(*'XVID'), 30, (frame_width, frame_height))\n",
    "\n",
    "time_step = 0\n",
    "while time_step < 1000:\n",
    "    # Get action from model\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # Render the environment and collect the frame\n",
    "    frame = env.render()\n",
    "    \n",
    "    # Convert RGB to BGR (OpenCV uses BGR format)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Write the frame into the video\n",
    "    video.write(frame)\n",
    "\n",
    "    time_step += 1\n",
    "\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "# Release the video writer\n",
    "video.release()\n",
    "\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Guardar el Modelo**\n",
    "\n",
    "Si el modelo ha mostrado un buen rendimiento, ¡asegúrate de guardarlo! Esto te permitirá usarlo más adelante sin tener que volver a entrenarlo.\n",
    "\n",
    "Puedes guardar el modelo con el siguiente comando en tu notebook:\n",
    "\n",
    "```python\n",
    "model.save(\"nombre_del_modelo\")\n",
    "```\n",
    "\n",
    "¡Así podrás seguir mejorando tu modelo o usarlo para otros experimentos en el futuro!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"locked_room_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
